---
title: "The Free-er Monad"
layout: single
permalink: /blog/freeer-monad/
---

## Introduction

Free monads allow you to represent effectful sequential programs as pure syntactic data, separate from their interpretation. You describe *what* should happen as an abstract tree of effects, leaving open *how* you want it to happen. By decoupling syntax from semantics in this way you gain full control over how programs are evaluated and interpreted - for example we could interpret a syntax tree in multiple ways:

- Run it directly
- Pretty print it
- Analyze it statically
- Compile to another language 
- Log or track resource use

Each of these corresponds to a different interpreter. This approach also allows effects to be combined without you having to get tangled up in monad transformers. *Freer* monads are a flexible generalization of free monads that make combining and interpreting effects even easier.

This post will introduce the freer monad in Lean — from categorical foundations to programming applications. We will cover what it means to be "free" in mathematics, what free monads are in categories and in code, and introduce freer monads as a solution to a certain type-theoretic obstacle to defining free monads in Lean. We will then use what we've learned to build and verify a small interpreter which makes elegant use of freeness to combine effectful computations.

This post assumes you know basic concepts from both category theory and functional programming, including functors, monads, and inductive datatypes.

## Free Objects

Before getting into free monads, we will first consider what it means for some structure to be "free". This section is pretty mathematical but if you're like me it may help understand things down the line when we start coding, otherwise you can skim this section.

As a familiar example, consider a vector space $V$ over a field $F$ with basis $B$. $V$ is said to be "free" on $B$ — it is the smallest vector space containing $B$, and is generated by taking all finite linear combinations of elements in $B$ with coefficients in $F$. A similar construction is done for other algebraic structures — for example the free group $G_S$ on a set $S$ consists of all the reduced words of elements of $S$.

The free object which is most familiar to the programmer is the free monoid on a datatype `α` — aka the type `List α`. This is of course the type of all finite sequences of elements of `α`, which forms the monoid `(List α, ++, [])`, where `++` is list concatenation.

How can we generalize this? In general categories, free constructions are defined as left adjoints to forgetful functors. If a forgetful functor $U : C \to D$ has a left adjoint $F : D \to C$, then for any $x \in D$, $Fx$ is the free $C$-object on $x$. 

Consider the functor $U : \texttt{Grp} \to \texttt{Set}$ which forgets the group structure and returns the underlying set. If $F : \texttt{Set} \to \texttt{Grp}$ is left adjoint to $U$, then $FX$ is the free group on $X$ for any set $X$.

This can also be stated in terms of a universal property. Slightly informally, an object $X'$ is the free $C$-object on $X$ if there is an embedding $\iota : X \to X'$ such that for any $C$-object $G$ with a map $h : X \to G$, there is a unique $C$-morphism $\hat{h} : X' \to G$ such that the following diagram commutes:

<div style="text-align: center;">
  <img src="https://raw.githubusercontent.com/tannerduve/tannerduve.github.io/master/images/tikz.png" alt="tikz diagram" />
</div>

*(Exercise: Let $U : \texttt{Grp} \to \texttt{Set}$ be the forgetful functor and $F$ a left adjoint. Prove $FX$ satisfies the above universal property for any set $X$.)*

The moral of the story here is that a free construction is the canonical way to generate the desired structure from some underlying data, adding only what is necessary to satisfy the rules of the desired structure. Our free monad is thus the canonical monad generated from an endofunctor, satisfying nothing other than the monad laws.

## Free Monads

> "A monad is just a monoid in the category of endofunctors, what's the problem?"

You’ve probably heard someone jokingly say monads are *just* monoids in the category of endofunctors. It's technically a correct definition, but it tells you nothing if you don't speak category theory. However, understanding monads as monoids may actually help us construct the free monad by analogy to the free monoid — aka the List type constructor. I know I promised this was an article on Lean but we will start with some Haskell first.

### In Haskell

The List type is defined as follows:

```haskell
data List a = Nil | Cons a (List a) 
```

Categorically this looks like:

$$
L_a \cong \mathbf{1} + (a \times L_a)
$$

i.e., the List type is a fixed point of the functor:

$$
F_a x = \mathbf{1} + (a \times x)
$$

(see: [inductive types as initial algebras](https://ncatlab.org/nlab/show/initial+algebra+of+an+endofunctor#more_examples))

Well, since the List functor maps a type to its free monoid, and we want our free monad functor to map an endofunctor to its free monad, we "lift" what we have done on lists in the category of types to free monads in the category of endofunctors. In this sense, lists are to types as free monads are to endofunctors. In programmer terms, we are defining a higher-order functor that is analogous to `List`, but acts on functors rather than types.

Our monoidal product in the endofunctor category is functor composition, the terminal object is the identity functor, and coproduct is defined component-wise. Thus the analogous functor is:

$$
\Phi_F G = \text{Id} + F \circ G
$$

Note how this corresponds to the above definition of `List`. Our free monad is the (least) fixed point:

$$
\text{Free}_F \cong \text{Id} + F \circ \text{Free}_F
$$

Finally we are all done with the category theory for this article. Let's now write our free monad in code:

```haskell
data Free f a = Pure a | Free (f (Free f a))
```

To be more explicit with the types, we have the constructors:

```haskell
Pure :: a -> Free f a
Free :: f (Free f a) -> Free f a
```

Convince yourself that this definition corresponds to the categorical one we gave above.

This already kind of looks like a monad by definition! Now, given that `f` is a functor, we can define a straightforward monad instance on `Free f` as follows:

```haskell
instance Functor f => Monad (Free f) where
  return = Pure
  Pure x >>= f = f x
  Free g >>= f = Free ((>>= f) <$> g)
```

### In Lean

Now, as promised, we will do the rest of our work in Lean. Let's write the same definition in Lean:

```lean
inductive Free (f : Type → Type) (a : Type) where
  | pure : a → Free f a
  | free : f (Free f a) → Free f a
```

Weird. We get this opaque error message:

```
-- error: (kernel) arg #3 of `Free.free' contains a non valid occurrence of the
datatypes being declared
```

Why does the definition work in Haskell but not Lean?

#### Strict Positivity

Recall that, in languages like Lean (or Coq, or Agda), in order for the proof system to be consistent, all functions must terminate. Proofs correspond to programs, and if we had programs that could loop forever, we could prove anything, and our logic would be useless.

To enforce this, defining inductive types has a restriction, called **strict positivity**. Strict positivity disallows contravariant occurrences of self-reference in the constructors of an inductive type. In simpler terms, an inductive type definition cannot refer to itself on the left side of an arrow in the constructors. If Lean allowed this definition, we could inhabit the empty type (i.e. prove false) using a contravariant functor.

Since the free monad doesn't work due to type-theoretic restrictions, we need a little bit more freedom. Enter: the freer monad. The below definition is strictly positive:

```lean
inductive Free (f : Type -> Type) (a : Type) where
  | pure : a -> Free f a
  | bind : ∀ x, f x -> (x -> Free f a) -> Free f a
```

In fact, this is *freer* in the sense that we no longer even require `f` to be a functor. Let's define the Functor and Monad instances for this type, given any type constructor.

#### Monad Instance of `Free f`

We begin by providing a Functor instance, which is just defining a map function, lifting a function $f : \alpha \to \beta$ to a function $Ff : \text{Free } F \ \alpha \to \text{Free } F \ \beta$:

```lean
def Free.map {a b : Type} (F : Type → Type) (f : a → b) : Free F a → Free F b :=
fun FFa =>
match FFa with
| pure a => Free.pure (f a)
| bind X Fx k => Free.bind X Fx (fun z => map F f (k z))

instance : Functor (Free F) where
map := Free.map F
```

Now we can provide a monad instance by defining `pure` and `bind` as follows:

```lean
def bindFree {a b : Type} (F : Type → Type) (x : Free F a) (f : a → Free F b) : Free F b :=
match x with
| .pure a => f a
| .bind X Fx k => .bind X Fx (fun z => bindFree F (k z) f)

instance FreeMonad (F : Type → Type) : Monad (Free F) where
  pure := Free.pure
  bind := bindFree F
```

Of course we all love Lean because you can actually prove things about the code you write. Lean provides not just a `Monad` typeclass, but a `LawfulMonad` typeclass, which additionally requires explicit proofs that the monad laws are satisfied. Let's do this for fun.

We first prove it is a lawful functor, i.e. it is *functorial* in the categorical sense:

- Identity law: $\text{map}\ id = id$
- Composition law: $\text{map}\ (g \circ f) = \text{map}\ g \circ \text{map}\ f$

```lean
instance : LawfulFunctor (Free F) where
map_const := by
  intro a b
  simp [Functor.mapConst, Functor.map]
id_map := by
  intro a x
  simp [Functor.map]
  induction x
  case pure a =>
    simp [Free.map]
  case bind X Fx f ih =>
    simp [Free.map, ih]
comp_map := by
  intro a b c g h x
  simp [Functor.map]
  induction x
  case pure a =>
    simp [Free.map]
  case bind X Fx f ih =>
    simp [Free.map, ih]
```

Now we prove that our structure is a **lawful monad**, meaning it satisfies the following **monad laws**:

* **Pure-Bind (Left identity):**

  $$
  \text{pure}(a) \gg= f = f(a)
  $$

* **Bind-Pure Composition (expresses `fmap` via `bind`):**  

  $$
  x \gg= (\lambda a.\, \text{pure}(f(a))) = \text{map}(f, x)   
  $$

* **Applicative Compatibility (expresses `<*>` via `bind`):**

  $$
  f \gg= (\lambda g.\, \text{map}(g, x)) = f \texttt{<*>} x
  $$

* **Bind Associativity:**

  $$
  (x \gg= f) \gg= g = x \gg= (\lambda a.\, f(a) \gg= g)
  $$

Because `Applicative` is a superclass of `Monad`, we must also verify the **applicative functor laws**:

* **Seq Left:**

  $$
  x \texttt{<*} y = \text{map}(\lambda x\, y.\, x, x) \texttt{<*>} y
  $$

* **Seq Right:**

  $$
  x \texttt{*>} y = \text{map}(\lambda x\, y.\, y, x) \texttt{<*>} y
  $$
    
* **Pure-Seq (expresses `fmap` via `<*>`):**

  $$
  \text{pure}(g) \texttt{<*>} x = \text{map}(g, x)
  $$

* **Map-Pure:**

  $$
  \text{map}(g, \text{pure}(x)) = \text{pure}(g(x))
  $$

* **Seq-Pure:**

  $$
  g \texttt{<*>} \text{pure}(x) = \text{map}(\lambda h.\, h(x), g)
  $$

* **Seq Associativity:**

  $$
  h \texttt{<*>} (g \texttt{<*>} x) = \text{map}(\text{comp}, h) \texttt{<*>} g \texttt{<*>} x
  $$

```lean
instance : LawfulMonad (Free F) where
bind_pure_comp := by
  intro α β x y; simp [Functor.map, bind, pure]; induction y
  · case pure a => simp [bindFree, Free.map]
  · case bind X Fx k ih => simp [bindFree, Free.map, ih]
bind_map := by 
  intro α β f x; simp [bind, Seq.seq]
pure_bind := by
  intro α x a f; simp [bind, pure, bindFree]
bind_assoc := by 
  intro α β γ x f g; simp [bind]; induction x 
  case pure a => simp [bindFree, Free.map]
  case bind X Fx k ih => simp [bindFree, Free.map, ih]
seqLeft_eq := by
  intro α β x y; simp [Functor.map, SeqLeft.seqLeft, Seq.seq]; induction x
  case pure a =>
    simp [bindFree, Free.map]
    induction y
    case pure b => simp [bindFree, Free.map]
    case bind X Fy k ih => simp [bindFree, Free.map, ih]
  case bind X Fx k ih => simp [bindFree, Free.map, ih]
seqRight_eq := by
  intro α β x y; simp [Functor.map, bindFree, Free.map]; induction x
  case pure a => 
    simp [bindFree, Free.map]
    induction y
    case pure b => simp [SeqRight.seqRight, Seq.seq, Functor.map, bindFree, Free.map]
    case bind X Fy k ih =>
      simp [SeqRight.seqRight, Seq.seq, Functor.map, bindFree, Free.map, ih] at ih ⊢ 
      apply funext; intro x; exact ih x
  case bind X Fx k ih =>
    simp [Free.map, Seq.seq, bindFree, Functor.map, SeqRight.seqRight] at ih ⊢
    apply funext; intro x; exact ih x 
pure_seq := by
  intro α β f x; simp [Seq.seq, Functor.map, pure, bindFree]
```
I won't write out the informal details of the proof, but it is mostly straightforward, we unfold all the definitions using `simp`, and in some cases when we have a value of type `Free F a` we perform induction on it and simplify further.

## An Interpreter with Side Effects

In this final section we will do a mini tutorial to show the power of the free monad by building an interpreter for an expression language with side effects. The key idea here is that the freer monad lets us separate what we want to do (a syntactic description of effectful computation) from how we want to do it (interpreting and executing the effects semantically).

### Language and Effects

We begin by defining a tiny expression language, with integers, variables, addition, and division:

```lean
inductive Expr where
  | val : Int → Expr
  | var : String → Expr
  | add : Expr → Expr → Expr
  | div : Expr → Expr → Expr
```

We also define three effect types: mutable state (for the environment), errors (for failed variable lookups or division by zero), and a trace log (for inspection or debugging):

```lean
inductive StateEff : Type → Type where
  | Get : StateEff Env
  | Put : Env → StateEff Unit

inductive ErrorEff : Type → Type where
  | Fail : String → ErrorEff Unit

inductive TraceEff : Type → Type where
  | Log : String → TraceEff Unit
```

We can then define a sum/coproduct of type constructors as follows:

```lean
inductive FSum (F G : Type → Type) (α : Type) where
  | inl : F α → FSum F G α
  | inr : G α → FSum F G α

infixl:50 "⊕" => FSum
```
And we define our overall effect signature as the nested sum:

```lean
abbrev Eff := StateEff ⊕ (ErrorEff ⊕ TraceEff)
```
Notice how free monads are extensible in their effects. Adding a new effect is simply constructing a new datatype and adding it to the Eff definition.

This type `Eff` is a pure description of the available commands in our language — not what they do, just what kinds of actions exist. Our computations will now live in the type `Free Eff α`, which means they are pure syntax trees of abstract effects that eventually return a value of type `α`.

### Lifting Effects into the Syntax Tree

To construct nodes in our effect AST, we define some helper functions that wrap each command in the Free monad:

```lean
def getEnv : Free Eff Env :=
  Free.bind _ (FSum.inl StateEff.Get) Free.pure

def putEnv (e : Env) : Free Eff Unit :=
  Free.bind _ (FSum.inl (StateEff.Put e)) Free.pure

def fail (msg : String) : Free Eff Unit :=
  Free.bind _ (FSum.inr (FSum.inl (ErrorEff.Fail msg))) Free.pure

def log (msg : String) : Free Eff Unit :=
  Free.bind _ (FSum.inr (FSum.inr (TraceEff.Log msg))) Free.pure
```

### Writing a Program

We can now write a little program. It logs a message, updates the environment, reads back a variable, and returns its increment:

```lean
def ex : Free Eff Int := do
  log "Starting"
  putEnv [("x", 10)]
  let env ← getEnv
  match env.find? (⋅.fst = "x") with
  | some (_, x) => pure (x + 1)
  | none => do fail "x not found"; pure 0
```
This "program" is constructing a tree of abstract effects independently of any execution or semantics. The calls to `log`, `putEnv`, `getEnv`, and fail are not doing anything yet — they are just nodes in a tree; entirely syntactic data. Programs are represented as data structures. When programs are represented as data structures you can do much more with them than just their operational interpretation, and you gain immense leverage and control over how you'd like to interpret them.

This separation between syntax and semantics is the core idea. We build up a value of type `Free Eff Int` that describes a program in terms of its desired behavior. This value is like an AST of effects. The tree is built using the constructors pure and bind, and the functorial action of the coproduct `⊕` lets us represent multiple kinds of effects simultaneously.

### Running the Program

To actually run the program, we define an interpreter — a recursive function that walks the syntax tree and gives meaning to each effect node. This function takes a `Free Eff α` value, along with an environment and a trace list, and evaluates the computation into an `Except String (α × Env × Trace)`:

```lean
def run {α : Type} : Free Eff α → Env → Trace → Except String (α × Env × Trace)
  | .pure a, env, trace => .ok (a, env, trace)
  | .bind _ fx k, env, trace =>
    match fx with
    | .inl sfx =>
      match sfx with
      | StateEff.Get => run (k env) env trace
      | StateEff.Put newEnv => run (k ()) newEnv trace
    | .inr sfx =>
      match sfx with
      | .inl efx =>
        match efx with
        | ErrorEff.Fail msg => .error msg
      | .inr tfx =>
        match tfx with
        | TraceEff.Log msg => run (k ()) env (trace ++ [msg])
```

Each clause handles one kind of effect and interprets it by modifying the environment, returning an error, or extending the trace. The tree is recursively traversed, with each node interpreted according to the semantics we now choose to assign.

This interpreter is just one way to give meaning to the syntax tree. Because effects are described abstractly, we can define different interpreters for different purposes — all working on the same underlying program. For example:

- A sandbox evaluator that runs the program with a mock environment and collects logs.
- A debugger that steps through effects interactively.
- A static analyzer that checks for possible failures without running the program.

This is the central idea of the freer monad pattern: build your program as a tree of abstract, uninterpreted commands. Delay all execution. Then define an interpreter that evalutes your programs however you want.

Here’s your revised **Verification** section with your new intro and consistent inlined formatting:

## Verification

Now that we have an interpreter, we can verify its correctness. What does correctness mean here?

In order to check that our interpreter is correct, we need some kind of semantics for our language, i.e., an assignment of meaning to our programs. In programming language theory, this is typically given by a **judgment** — a formal relation that specifies when evaluation succeeds and what result it produces.

We’ll define a *big-step operational semantics* as an inductive relation, and then prove that the interpreter agrees with the semantics.

### What does it mean to evaluate an expression?

We define a relation `EvalRel e env trace res` that says: under environment `env` and trace `trace`, expression `e` evaluates to result `res`. This result is either an error or a triple of the resulting value, environment, and trace. Our correctness claim will then be that if `EvalRel e env trace res` holds (i.e., `e` evaluates to `res`), then our interpreter also returns `res` when run on the output of `eval e`.

```lean
inductive EvalRel : Expr → Env → Trace → Except String (Int × Env × Trace) → Prop where
| val :
    ∀ n env trace,
    EvalRel (.val n) env trace (.ok (n, env, trace))
| var_found :
    ∀ x env trace v,
    env.find? (·.fst = x) = some (x, v) →
    EvalRel (.var x) env trace (.ok (v, env, trace))
| var_missing :
    ∀ x env trace,
    env.find? (·.fst = x) = none →
    EvalRel (.var x) env trace (.error s!"unbound variable {x}")
| add :
    ∀ e1 e2 env trace₁ trace₂ trace₃ v1 v2 env₂ env₃,
    EvalRel e1 env trace₁ (.ok (v1, env₂, trace₂)) →
    EvalRel e2 env₂ trace₂ (.ok (v2, env₃, trace₃)) →
    EvalRel (.add e1 e2) env trace₁ (.ok (v1 + v2, env₃, trace₃))
| div_ok :
    ∀ e1 e2 env trace₁ trace₂ trace₃ v1 v2 env₂ env₃,
    v2 ≠ 0 →
    EvalRel e1 env trace₁ (.ok (v1, env₂, trace₂)) →
    EvalRel e2 env₂ trace₂ (.ok (v2, env₃, trace₃)) →
    EvalRel (.div e1 e2) env trace₁ (.ok (v1 / v2, env₃, trace₃))
| div_zero :
    ∀ e1 e2 env trace₁ trace₂ trace₃ v1 v2 env₂ env₃,
    v2 = 0 →
    EvalRel e1 env trace₁ (.ok (v1, env₂, trace₂)) →
    EvalRel e2 env₂ trace₂ (.ok (v2, env₃, trace₃)) →
    EvalRel (.div e1 e2) env trace₁ (.error "divide by zero")
```

### What is `eval`?

The function `eval : Expr → Free Eff Int` maps an expression into our effectful AST. It constructs a tree of effects representing what should happen during evaluation. This is the object our interpreter consumes.

```lean
def eval : Expr → Free Eff Int
  | .val n => pure n
  | .var x => do
      let env ← getEnv
      match env.find? (·.fst = x) with
      | some (_, v) => pure v
      | none => do
          fail s!"unbound variable {x}"
          pure 0
  | .add e1 e2 => do
      let v1 ← eval e1
      let v2 ← eval e2
      pure (v1 + v2)
  | .div e1 e2 => do
      let v1 ← eval e1
      let v2 ← eval e2
      if v2 = 0 then
        fail "divide by zero"
        pure 0
      else
        pure (v1 / v2)
```

### What do we want to prove?

We want to prove that `eval` followed by `run` gives the same result as the semantics. That is:

```lean
theorem eval_correct :
  ∀ (e : Expr) (env : Env) (trace : Trace) (res : Except String (Int × Env × Trace)),
    EvalRel e env trace res →
    run (eval e) env trace = res
```

### Proof sketch

We proceed by induction on the derivation of `EvalRel e env trace res`. In each case, we:

* Unfold the definition of `eval` for the given expression
* Use helper lemmas to simplify `run (p >>= k)`
* Match the result with the expected output

These two helper lemmas simplify `run (p >>= k)`:

```lean
theorem run_bind_ok {α β}
    {p : Free Eff α} {k : α → Free Eff β}
    {env env' : Env} {tr tr' : Trace} {v : α} :
  run p env tr = .ok (v, env', tr') →
  run (p >>= k) env tr = run (k v) env' tr' := ...
```

If `p` succeeds with `v`, then `p >>= k` runs `k v` next.

```lean
theorem run_bind_err {α β}
    {p : Free Eff α} {k : α → Free Eff β}
    {env : Env} {tr : Trace} {msg : String} :
  run p env tr = .error msg →
  run (p >>= k) env tr = .error msg := ...
```

If `p` errors, then `p >>= k` errors with the same message.

Now we can prove the theorem.

```lean
theorem eval_correct :
  ∀ (e : Expr) (env : Env) (trace : Trace) (res : Except String (Int × Env × Trace)),
    EvalRel e env trace res →
    run (eval e) env trace = res := by
  intro e env trace res h
  induction h
  case val n env' trace' =>
    simp [eval, pure, run]
  case var_found x env' trace' v hfind =>
    simp [eval, getEnv, run, bind, bindFree, FSum.inl, hfind]
  case var_missing x' env' t hfind =>
    simp [eval, getEnv, run, bind, bindFree, FSum.inl, hfind, Eff, fail, FSum.inr]
  case add e1 e2 env' trace₁ trace₂ trace₃ v1 v2 env₂ env₃ he1 he2 ih₁ ih₂ =>
    simp [eval, bind]
    have step₁ := run_bind_ok (p := eval e1) (k := fun v1 => do let v2 ← eval e2; pure (v1 + v2)) ih₁
    simp [bind] at step₁; simp [step₁]
    have step₂ := run_bind_ok (p := eval e2) (k := fun v2 => pure (v1 + v2)) ih₂
    simp [bind] at step₂; simp [step₂, run]
  case div_ok e1 e2 env' trace₁ trace₂ trace₃ v1 v2 env₂ env₃ hne he1 he2 ih₁ ih₂ =>
    simp [eval, bind]
    have step₁ := run_bind_ok (p := eval e1) (k := fun v1 => do let v2 ← eval e2; if v2 = 0 then fail "divide by zero"; pure 0 else pure (v1 / v2)) ih₁
    simp [bind] at step₁; simp [step₁]
    have step₂ := run_bind_ok (p := eval e2) (k := fun v2 => if v2 = 0 then fail "divide by zero"; pure 0 else pure (v1 / v2)) ih₂
    simp [bind] at step₂; simp [step₂, hne]; simp [run]
  case div_zero e1 e2 env' trace₁ trace₂ trace₃ v1 v2 env₂ env₃ heq he1 he2 ih₁ ih₂ =>
    simp [eval, bind]
    have step₁ := run_bind_ok (p := eval e1) (k := fun v1 => do let v2 ← eval e2; if v2 = 0 then fail "divide by zero"; pure 0 else pure (v1 / v2)) ih₁
    simp [bind] at step₁; simp [step₁]
    have step₂ := run_bind_ok (p := eval e2) (k := fun v2 => if v2 = 0 then fail "divide by zero"; pure 0 else pure (v1 / v2)) ih₂
    simp [bind] at step₂; simp [step₂, heq]; simp [fail, run, bindFree]
```

Now we have formally verified our interpreter agrees with our language semantics.

## Conclusion 

Hopefully this article was informative and helpful in understanding free monads mathematically and gave you a glimpse of their usefulness in programming. This is the first blog post I've written so I'm hoping it was enjoyable. To review what we did:

- We introduced the concept of free objects in mathematics, starting with vector spaces, monoids, and groups.

- We defined the free monad categorically as the lease fixed point of a particular functor, drawing analogy to the List type.

- In Haskell, we implemented the standard `Free f a` type and gave it a Monad instance.

- We learned about strict positivity in dependently typed proof assistants and why the classic `Free` monad fails in Lean

- We introduced the Freer monad as a strictly positive solution, and showed it forms a monad for any `F : Type -> Type`

- We defined a small expression language with three effects: state, errors, and tracing, showed how effects can be represented as data structures using the Free monad, and wrote an interpreter for it

- We showed how this separation between syntax and semantics enables flexibility in evaluating and interpreting effectful languages.

- We showed how to define an operational semantics and prove that the interpreter agrees with it.

## Exercises
- Write an interpreter that counts how many times each effect is used in a program.
- Define the standard monads such as `State`, `Writer`, and `Reader` as `Free` monads.
- Define the dual notion - the *cofree comonad* and explore its properties.

## References

* [nLab: Free Monad](https://ncatlab.org/nlab/show/free+monad)

* [CIS 5520 Lecture Notes on Freer Monads](https://www.seas.upenn.edu/~cis5520/22fa/lectures/stub/11-transformers/Freer.html)

* *The Dao of Functional Programming*, Bartosz Milewski (2025)

* [Serokell: Introduction to Free Monads](https://serokell.io/blog/introduction-to-free-monads)

* [Okmij : Free and Freer Monads: Putting Monads Back into Closet](https://okmij.org/ftp/Computation/free-monad.html)